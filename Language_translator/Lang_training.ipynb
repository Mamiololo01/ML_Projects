{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tensorflow\n",
    "import sklearn\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers import Input,LSTM,Dense\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#initialize all variables \n",
    "input_texts=[]\n",
    "target_texts=[]\n",
    "input_characters=set()\n",
    "target_characters=set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse the dataset file\n",
    "\n",
    "We will traverse the dataset file and extract all the input and target texts. For this, we will be using the first 10,000 rows of our dataset for the training and testing part. It can be changed as per requirements. To separate input and target texts from the row we will use ‘\\t’ and to separate rows we will use ‘\\n’.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read dataset file\n",
    "with open('eng-french.txt','r',encoding='utf-8') as f:\n",
    "    rows=f.read().split('\\n')\n",
    "#read first 10,000 rows from dataset     \n",
    "for row in rows[:10000]:\n",
    "    #split input and target by '\\t'=tab\n",
    "    input_text,target_text = row.split('\\t')\n",
    "    #add '\\t' at start and '\\n' at end of text.\n",
    "    target_text='\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text.lower())\n",
    "    target_texts.append(target_text.lower())\n",
    "    #split character from text and add in respective sets\n",
    "    input_characters.update(list(input_text.lower()))\n",
    "    target_characters.update(list(target_text.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same procedure and separate the text from rows and characters. Also, get the maximum length of encoder as well as decoder sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of encoder characters :  47\n",
      "number of decoder characters :  67\n",
      "maximum input length :  16\n",
      "maximum target length :  59\n"
     ]
    }
   ],
   "source": [
    "#sort input and target characters \n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "\n",
    "#get the total length of input and target characters\n",
    "num_en_chars = len(input_characters)\n",
    "num_dec_chars = len(target_characters)\n",
    "\n",
    "#get the maximum length of input and target text.\n",
    "max_input_length = max([len(i) for i in input_texts])\n",
    "max_target_length = max([len(i) for i in target_texts])\n",
    "\n",
    "print(\"number of encoder characters : \",num_en_chars)\n",
    "print(\"number of decoder characters : \",num_dec_chars)\n",
    "\n",
    "print(\"maximum input length : \",max_input_length)\n",
    "print(\"maximum target length : \",max_target_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models cannot work directly on the categorical data. For this, we require one hot encoding process. One-hot encoding deals with the data in binary format so we encode the categorical data in binary format.\n",
    "\n",
    "One-hot means that we can only make an index of data 1 (true) if it is present in the vector or else 0 (false). So every data has its unique representation in vector format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagofcharacters(input_texts,target_texts):\n",
    "\n",
    "    #initialize encoder , decoder input and target data.\n",
    "    en_in_data=[] ; dec_in_data=[] ; dec_tr_data=[]\n",
    "\n",
    "    #padding variable with first character as 1 as rest all 0.\n",
    "    pad_en=[1]+[0]*(len(input_characters)-1)\n",
    "    pad_dec=[0]*(len(target_characters)) ; pad_dec[2]=1\n",
    "\n",
    "    #countvectorizer for one hot encoding as we want to tokenize character so\n",
    "    #analyzer is true and None the stopwords action.\n",
    "    cv=CountVectorizer(binary=True,tokenizer=lambda txt:\n",
    "    txt.split(),stop_words=None,analyzer='char')\n",
    "  \n",
    "    for i,(input_t,target_t) in enumerate(zip(input_texts,target_texts)):\n",
    "        #fit the input characters into the CountVectorizer function\n",
    "        cv_inp= cv.fit(input_characters)\n",
    "    \n",
    "        #transform the input text from the help of CountVectorizer fit.\n",
    "        #it character present than put 1 and 0 otherwise.\n",
    "        en_in_data.append(cv_inp.transform(list(input_t)).toarray().tolist())\n",
    "        cv_tar= cv.fit(target_characters)    \n",
    "        dec_in_data.append(cv_tar.transform(list(target_t)).toarray().tolist())\n",
    "\n",
    "        #decoder target will be one timestep ahead because it will not consider \n",
    "        #the first character i.e. '\\t'.\n",
    "        dec_tr_data.append(cv_tar.transform(list(target_t)[1:]).toarray().tolist())\n",
    "          #add padding variable if the length of the input or target text is smaller\n",
    "  #than their respective maximum input or target length. \n",
    "    if len(input_t) < max_input_length:\n",
    "      for _ in range(max_input_length-len(input_t)):\n",
    "        en_in_data[i].append(pad_en)\n",
    "    if len(target_t) < max_target_length:\n",
    "      for _ in range(max_target_length-len(target_t)):\n",
    "        dec_in_data[i].append(pad_dec)\n",
    "    if (len(target_t)-1) < max_target_length:\n",
    "      for _ in range(max_target_length-len(target_t)+1):\n",
    "        dec_tr_data[i].append(pad_dec)\n",
    "\n",
    "#convert list to numpy array with data type float32\n",
    "    en_in_data=np.array(en_in_data,dtype=\"float32\")\n",
    "    dec_in_data=np.array(dec_in_data,dtype=\"float32\")\n",
    "    dec_tr_data=np.array(dec_tr_data,dtype=\"float32\")\n",
    "    return en_in_data,dec_in_data,dec_tr_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the training model\n",
    "\n",
    "In this language translation project, we will be using LSTM to train our machine learning model to translater language. LSTM (Long Short Term Memory) network: LSTM is a type of RNN (Recurrent Neural Network) that solves scenarios where RNN is failed.\n",
    "\n",
    "Long-Term Dependency: In RNN, networks have the data of previous output in memory for a short period of time because of this they are unaware about the actual context of the sentence over a long period of time. This raised the issue of long-term dependency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create input object of total number of encoder characters\n",
    "en_inputs = Input(shape=(None, num_en_chars))\n",
    "\n",
    "#create LSTM with the hidden dimension of 256\n",
    "#return state=True as we don't want output sequence.\n",
    "encoder = LSTM(256, return_state=True)\n",
    "\n",
    "#discard encoder output and store hidden and cell state.\n",
    "en_outputs, state_h, state_c = encoder(en_inputs)\n",
    "en_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create input object of total number of decoder characters\n",
    "dec_inputs = Input(shape=(None, num_dec_chars))\n",
    "\n",
    "#create LSTM with the hidden dimension of 256\n",
    "#return state and return sequences as we want output sequence.\n",
    "dec_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "\n",
    "#initialize the decoder model with the states on encoder.\n",
    "dec_outputs, _, _ = dec_lstm(dec_inputs, initial_state=en_states)\n",
    "\n",
    "#Output layer with shape of total number of decoder characters \n",
    "dec_dense = Dense(num_dec_chars, activation=\"softmax\")\n",
    "dec_outputs = dec_dense(dec_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    "To train the model we will fit ‘(encoder input and decoder input)’ which will turn into (‘decoder target data’) using ‘Adam’ optimizer with a validation split of 0.2 and provide an epoch of 200 in a batch size of 64. Also, we will store all required variables in a binary or bytes stream like object format file using the ‘pickle’ module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christopherozurumba/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/christopherozurumba/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:563: UserWarning: The parameter 'tokenizer' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (10000,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m pickle\u001b[38;5;241m.\u001b[39mdump({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_characters\u001b[39m\u001b[38;5;124m'\u001b[39m:input_characters,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_characters\u001b[39m\u001b[38;5;124m'\u001b[39m:target_characters, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_input_length\u001b[39m\u001b[38;5;124m'\u001b[39m:max_input_length, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_target_length\u001b[39m\u001b[38;5;124m'\u001b[39m:max_target_length, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_en_chars\u001b[39m\u001b[38;5;124m'\u001b[39m:num_en_chars, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_dec_chars\u001b[39m\u001b[38;5;124m'\u001b[39m:num_dec_chars}, \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_data.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#load the data and train the model\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m en_in_data,dec_in_data,dec_tr_data \u001b[38;5;241m=\u001b[39m bagofcharacters(input_texts,target_texts)\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m      9\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m     13\u001b[0m     [en_in_data, dec_in_data],\n\u001b[1;32m     14\u001b[0m     dec_tr_data,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m     validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[1;32m     18\u001b[0m )\n",
      "Cell \u001b[0;32mIn[22], line 41\u001b[0m, in \u001b[0;36mbagofcharacters\u001b[0;34m(input_texts, target_texts)\u001b[0m\n\u001b[1;32m     38\u001b[0m         dec_tr_data[i]\u001b[38;5;241m.\u001b[39mappend(pad_dec)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m#convert list to numpy array with data type float32\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     en_in_data\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(en_in_data,dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     42\u001b[0m     dec_in_data\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(dec_in_data,dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m     dec_tr_data\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(dec_tr_data,dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (10000,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "#create Model and store all variables \n",
    "model = Model([en_inputs, dec_inputs], dec_outputs)\n",
    "\n",
    "pickle.dump({'input_characters':input_characters,'target_characters':target_characters, 'max_input_length':max_input_length, 'max_target_length':max_target_length, 'num_en_chars':num_en_chars, 'num_dec_chars':num_dec_chars}, open(\"training_data.pkl\", \"wb\"))\n",
    "\n",
    "#load the data and train the model\n",
    "en_in_data,dec_in_data,dec_tr_data = bagofcharacters(input_texts,target_texts)\n",
    "model.compile(\n",
    "    optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    [en_in_data, dec_in_data],\n",
    "    dec_tr_data,\n",
    "    batch_size=64,\n",
    "    epochs=200,\n",
    "    validation_split=0.2,\n",
    ")\n",
    "\n",
    "# Save model\n",
    "model.save(\"s2s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
